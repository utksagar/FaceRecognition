{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "from sklearn.externals import joblib\n",
    "import cv2\n",
    "from IPython import display\n",
    "import signal\n",
    "import matplotlib.patches as patches\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDemo(object):\n",
    "    def __init__(self, cascade_path):\n",
    "        self.vc = None\n",
    "        self.cascade = cv2.CascadeClassifier(cascade_path)\n",
    "        self.margin = 10\n",
    "        self.is_interrupted = False\n",
    "        self.le = None\n",
    "        self.clf = None\n",
    "        \n",
    "    def _signal_handler(self, signal, frame):\n",
    "        self.is_interrupted = True\n",
    "        \n",
    "    def infer(self):\n",
    "        vc = cv2.VideoCapture(0)\n",
    "        self.vc = vc\n",
    "        if vc.isOpened():\n",
    "            is_capturing, _ = vc.read()\n",
    "        else:\n",
    "            is_capturing = False\n",
    "\n",
    "        signal.signal(signal.SIGINT, self._signal_handler)\n",
    "        self.is_interrupted = False\n",
    "        while is_capturing:\n",
    "            is_capturing, frame = vc.read()\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            faces = self.cascade.detectMultiScale(frame,\n",
    "                                         scaleFactor=1.1,\n",
    "                                         minNeighbors=3,\n",
    "                                         minSize=(100, 100))\n",
    "            pred = None\n",
    "            if len(faces) != 0:\n",
    "                face = faces[0]\n",
    "                (x, y, w, h) = face\n",
    "                left = x - self.margin // 2\n",
    "                right = x + w + self.margin // 2\n",
    "                bottom = y - self.margin // 2\n",
    "                top = y + h + self.margin // 2\n",
    "                img = resize(frame[bottom:top, left:right, :],\n",
    "                             (160, 160), mode='reflect')\n",
    "                embs = custom_model.predict(np.expand_dims(img, axis=0))\n",
    "                pred = self.le.inverse_transform(self.clf.predict(embs))\n",
    "                cv2.rectangle(frame,\n",
    "                              (left-1, bottom-1),\n",
    "                              (right+1, top+1),\n",
    "                              (255, 0, 0), thickness=2)\n",
    "            plt.imshow(frame)\n",
    "            plt.title(pred)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            display.clear_output(wait=True)\n",
    "            try:\n",
    "                plt.pause(0.1)\n",
    "            except Exception:\n",
    "                pass\n",
    "            if self.is_interrupted:\n",
    "                vc.release()\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/anaconda3/envs/rudra/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "custom_model = load_model(\"./models/saved/mtcnn/custome_FR_Inceptionv1_512D_mtcnn.h5\")\n",
    "knn_classifier = joblib.load(\"./models/saved/mtcnn/knn.joblib\")\n",
    "encoder = joblib.load(\"./models/saved/mtcnn/encoder.joblib\")\n",
    "#custom_model.summary()\n",
    "cascade_path = \"./models/haarcascade_frontalface_alt2.xml\"\n",
    "cascade = cv2.CascadeClassifier(cascade_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facedlib = FaceDemo(cascade_path)\n",
    "facedlib.le = encoder\n",
    "facedlib.clf = knn_classifier\n",
    "facedlib.infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instead of using harcascade use MTCNN(multitasking cascading CNN) for face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDemoMTCNN(object):\n",
    "    def __init__(self, cascade_path):\n",
    "        self.vc = None\n",
    "        self.cascade = cv2.CascadeClassifier(cascade_path)\n",
    "        self.margin = 10\n",
    "        self.is_interrupted = False\n",
    "        self.le = None\n",
    "        self.clf = None\n",
    "        \n",
    "    def _signal_handler(self, signal, frame):\n",
    "        self.is_interrupted = True\n",
    "        \n",
    "    def infer(self):\n",
    "        vc = cv2.VideoCapture(0)\n",
    "        self.vc = vc\n",
    "        if vc.isOpened():\n",
    "            is_capturing, _ = vc.read()\n",
    "        else:\n",
    "            is_capturing = False\n",
    "\n",
    "        signal.signal(signal.SIGINT, self._signal_handler)\n",
    "        self.is_interrupted = False\n",
    "        from align_mtcnn import AlignMTCNN\n",
    "        align_image_mtcnn = AlignMTCNN()\n",
    "        pnet, rnet, onet = align_image_mtcnn.get_session(0.1)\n",
    "        while is_capturing:\n",
    "            is_capturing, frame = vc.read()\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            faces = align_image_mtcnn.detect_face(frame, pnet, rnet, onet, largetsbb=True)\n",
    "            pred = None\n",
    "            if len(faces) != 0:\n",
    "                face = faces[0]\n",
    "                (x, y, w, h) = face\n",
    "                left = int(round(x)) - self.margin // 2\n",
    "                right = int(round(w)) + self.margin // 2\n",
    "                bottom = int(round(y)) - self.margin // 2\n",
    "                top =  int(round(h)) + self.margin // 2\n",
    "                print(\"xfkjgkjdf\", bottom,top, left,right)\n",
    "                img = resize(frame[bottom:top, left:right, :],\n",
    "                             (160, 160), mode='reflect')\n",
    "                embs = custom_model.predict(np.expand_dims(img, axis=0))\n",
    "                pred = self.le.inverse_transform(self.clf.predict(embs))\n",
    "                cv2.rectangle(frame,\n",
    "                              (left-1, bottom-1),\n",
    "                              (right+1, top+1),\n",
    "                              (255, 0, 0), thickness=2)\n",
    "            plt.imshow(frame)\n",
    "            plt.title(pred)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            display.clear_output(wait=True)\n",
    "            try:\n",
    "                plt.pause(0.1)\n",
    "            except Exception:\n",
    "                pass\n",
    "            if self.is_interrupted:\n",
    "                vc.release()\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facemtcnn = FaceDemoMTCNN(cascade_path)\n",
    "facemtcnn.le = encoder\n",
    "facemtcnn.clf = knn_classifier\n",
    "facemtcnn.infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/anaconda3/envs/rudra/lib/python3.6/site-packages/keras/engine/saving.py:292: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from sklearn.externals import joblib\n",
    "import align\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "img_path = \"data/test/230220.png\"\n",
    "clf_path = \"./models/saved/knn.joblib\"\n",
    "le_path = \"./models/saved/encoder.joblib\"\n",
    "face_detector = \"models/shape_predictor_68_face_landmarks.dat\"\n",
    "\n",
    "def load_img(img_path):\n",
    "    img = cv2.imread(img_path, 1)\n",
    "    return img[..., ::-1]\n",
    "\n",
    "def align_image(img, face_detector):\n",
    "    aligndlib = align.AlignDlib(face_detector)\n",
    "    return aligndlib.align(160, img, aligndlib.getLargestFaceBoundingBox(img),\n",
    "                               landmarkIndices=align.AlignDlib.OUTER_EYES_AND_NOSE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"models/facenet_keras.h5\")\n",
    "model.load_weights(\"models/facenet_keras_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def classify(img_path, clf_path, le_path, model, face_detector):\n",
    "img = load_img(img_path)\n",
    "clf = joblib.load(clf_path)\n",
    "encoder = joblib.load(le_path)\n",
    "aligned_image = align_image(img, face_detector)\n",
    "aligned_image = (aligned_image / 255.).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 160, 160, 3)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(aligned_image,axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = []\n",
    "for index, i in enumerate(range(10)):\n",
    "    images.append(aligned_image)\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.asarray(images)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = model.predict(z)\n",
    "# emb = model.predict(np.expand_dims(aligned_image, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emb.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = model.predict([np.expand_dims(aligned_image, axis=0)])[0]\n",
    "pred = clf.predict([emb])\n",
    "prediction = encoder.inverse_transform(pred)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 5, 5, 5, 5, 5, 5, 5, 5])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/admin/anaconda3/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rudra)",
   "language": "python",
   "name": "rudra"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
